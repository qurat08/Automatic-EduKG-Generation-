# CourseMapper-KnowledgeGraph

CourseMapper KG project

## Installation

### Docker Installation

#### Download

- ELMo: Download links here: [Link 1](https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5) and [Link 2](https://uni-duisburg-essen.sciebo.de/s/r4bNsDrkuAkPSfo/download).
  Save the files in the `auxiliary_data/` directory.
- StanfordCoreNLP: Download link [here](https://uni-duisburg-essen.sciebo.de/s/gSQZfKdd1mJy80P/download).
  Unzip and save in the project directory.

### Docker

The project can be installed with docker by executing:

```
docker-compose up
```

Containers for the neo4j database and the application will created.

## Manual installation (outdated)

#### Dependencies

```python
pip install -r requirements.txt
```

#### Neo4j database

- Create neo4j database
- Set `NEO4J_URI`, `NEO4J_USER` and `NEO4J_PW` in `.env` file
- Run neo4j cypher file in `db/neo4j-cyphers` to create constraints

## Run application

```python
python api/app.py
```

## Modules

The differents steps involved in the construction of the KG are listed below with their description.

### Keyphrase Extraction

A lightweight configurable keyphrase extraction model based on pretrained transformers word embeddings to compute token embeddings and SIF (Smooth Inverse Frequency) for the sentence embedding.

The model is based on [SIFRank](https://github.com/sunyilgdx/SIFRank).

#### Usage

###### Elmo

```python
from kwp_extraction.model import KeyphraseExtractor

text = """
In object-oriented programming, a class is an extensible program-code-template for creating
objects, providing initial values for state (member variables) and implementations of behavior
(member functions or methods). In many languages, the class name is used as the name for the
class (the template itself), the name for the default constructor of the class (a subroutine
that creates objects), and as the type of objects generated by instantiating the class;
these distinct concepts are easily conflated.
"""

kp_extractor = KeyphraseExtractor()
keyphrases = kp_extractor.extract_keyphrases(text=text, top_n=15)
```

###### Transformers Word Embeddings

Transformer models can be downloaded from [HuggingFace](https://huggingface.co/transformers/pretrained_models.html)

```python
from flair.embeddings import TransformerWordEmbeddings
model = TransformerWordEmbeddings("squeezebert/squeezebert-mnli")
kp_extractor = KeyphraseExtractor(model)

keyphrases = kp_extractor.extract_keyphrases(text)
```

Instead of using the StanfordCoreNLP library, flair POS tagging models can be used.
The models can be found [here](https://huggingface.co/flair). Select only english models.

```python
kp_extractor = KeyphraseExtractor("squeezebert/squeezebert-mnli",
                                   tagger_model='flair/pos-english')
```

The result is a list of ranked keyphrases:

```python
>>> keyphrases
[('member functions', 0.8907741637952991),
('initial values', 0.8812833951381089),
('member variables', 0.8801865326481915),
('object', 0.8695915432056264),
('class name', 0.8624032114420161),
('extensible program-code-template', 0.8525536811098724),
('object-oriented programming', 0.852514713060075),
('default constructor', 0.8464177375216444),
('behavior', 0.843253352277783),
('class', 0.8381748085298),
('method', 0.8161418276957679),
('subroutine', 0.8063567379716012),
('distinct concepts', 0.7950399202257336),
('implementation', 0.793406249110016),
('state', 0.7728810507312818)]
```

### Concept Identification & Expansion & Weigthing

Concepts are identified from either the keyphrases or the text of the learning material.
DBpedia Spotlight is used to annotate DBpedia concepts.

The expansion of the KG is done by adding for each annotated concept, the categories and the related concepts. Categories and related concepts are collected from DBpedia.

Each node (concept, category and related concept) in the KG is weighted.
Weigting score of a concept, related concept and category is the cosine similarity between the text of the learning material and the content of the wikipedia article for the concept, the content of the wikipedia article for the related concept, and the name of the category, respectively.

All these steps will result in a collection of weighted nodes (concepts, related concepts and categories) with various attributes.

The attributes are:

- id: the id of the node
- name: the name of the node
- uri: the DBpedia uri of the node
- type: wether a node is a concept ("annotation"), a related concept ("property") or a category ("category")
- mid: the id of the learning material
- to: the relations of the node
- weight: the weight of the node
- wikipedia: the wikipedia link of the node (only for nodes of type concept and related concept)
- abstract: the wikipedia summary of a node (only for nodes of type concept and related concept)

```python
from kwp_extraction.dbpedia.concept_tagging import DBpediaSpotlight

dbpedia = DBpediaSpotlight()

concepts = dbpedia.build_path(materialId="001",
                              text=text,
                              keyphrases=keyphrases
)
```

By default, the extension is done for categories (`with_category`)and related concepts (`with_property`).

`with_doc_sim` enables weight calculation for the nodes.

`whole_text`allows identification (or annotation) on either keyphrases (`False`) or the text of the learning material (`True`).

```python
>>> dbpedia.build_path(materialId="001",
                              text=text,
                              keyphrases=keyphrases,
                              with_category=True,
                              with_property=True,
                              with_doc_sim=True,
                              whole_text=False
)
```

The result is collection of weighted concepts, related concepts and categories.

```python
[
  { "id": 7134846275315333605,
    "name": "Statistical randomness",
    "uri": "http://dbpedia.org/resource/Category:Statistical_randomness",
    "type": "category",
    "mid": "001",
    "to": [],
    "weight": 0.14,
    "wikipedia": "",
    "abstract": ""
  },
  { "id": 5055752578680548232,
    "label": "variables",
    "uri": "http://dbpedia.org/resource/Random_variable",
    "sim_score": "0.564253161427046",
    "type": "annotation",
    "mid": "001",
    "to": [
       { "id": 7134846275315333605,
         "name": "Statistical randomness",
         "weight": 0.7647058823529411,
         "rel_type": "BELONGS_TO"
       },
       { "id": 8087619363668187169,
         "name": "Independent and identically distributed random variables",
         "weight": 0.8181818181818181,
         "rel_type": "RELATED_TO"
       },
      ...
  }
  ...
]
```

##### References

- [https://github.com/sunyilgdx/SIFRank](https://github.com/sunyilgdx/SIFRank)
- [https://github.com/MaartenGr/KeyBERT](https://github.com/MaartenGr/KeyBERT)
